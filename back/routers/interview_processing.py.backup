import json
import os
import sqlite3
import assemblyai as aai
from fastapi import APIRouter, HTTPException, WebSocket, WebSocketDisconnect, Query
from fastapi.responses import JSONResponse
from database import get_db_connection
import asyncio
from datetime import datetime
from openai import AsyncOpenAI
import aiofiles
from dotenv import load_dotenv
import websockets
import base64
import struct

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ASSEMBLYAI_API_KEY = os.getenv("ASSEMBLYAI_API_KEY")

aai.settings.api_key = ASSEMBLYAI_API_KEY
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)

router = APIRouter(
    prefix="/positions/interviews",
    tags=["Interview Processing"]
)


with open("prompts/prompt_analitico.txt", "r") as f:
    prompt_template = f.read()

@router.patch("/{id}/process/analysis")
async def generate_analysis(id: int):
    print(f"[DEBUG] Endpoint /process/analysis chamado para interview ID: {id}")
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(
        """
            SELECT
                interviews.transcript,
                interviews.notes,
                positions.position AS position,
                positions.skills AS skills,
                positions.description AS description
            FROM interviews
            JOIN positions ON interviews.position_id = positions.id
            WHERE interviews.id = ?
        """,
        (id,)
    )
    row = cursor.fetchone()
    if not row:
        conn.close()
        print(f"[ERROR] Interview {id} não encontrado no banco")
        raise HTTPException(status_code=404, detail="Entrevista não encontrada")
    if not row["transcript"]:
        conn.close()
        print(f"[ERROR] Transcript não encontrado para interview {id}")
        raise HTTPException(status_code=404, detail="Transcrição não encontrada")
    interview_info = {
        "position_data": {
            "position": row["position"],
            "skills": json.loads(row["skills"]),
            "description": row["description"]
        },
        "transcript": json.loads(row["transcript"]),
        "notes": row["notes"]
    }
    info_json = json.dumps(interview_info)

    prompt_final = prompt_template + info_json

    print(f"[DEBUG] Iniciando geração de análise para interview {id}")
    print(f"[DEBUG] Tamanho do prompt: {len(prompt_final)} caracteres")
    
    try:
        response = await asyncio.wait_for(
            openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt_final}],
        response_format={ "type": "json_object" },
        temperature=0.5
            ),
            timeout=120.0  # Timeout de 2 minutos
        )
        print(f"[DEBUG] Análise gerada com sucesso para interview {id}")
    except asyncio.TimeoutError:
        conn.close()
        print(f"[ERROR] Timeout ao gerar análise para interview {id}")
        raise HTTPException(status_code=504, detail="Timeout ao gerar análise. Tente novamente.")
    except Exception as e:
        conn.close()
        print(f"[ERROR] Erro ao gerar análise para interview {id}: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao gerar análise: {str(e)}")
    json_gerado = response.choices[0].message.content
    dictionary = json.loads(json_gerado)
    cursor.execute("UPDATE interviews SET analysis = ?, score = ? WHERE id = ?", (json_gerado, dictionary["score"]["overall"], id))
    conn.commit()
    conn.close()
    return JSONResponse(content={"id": id, "analysis": dictionary, "message": "Resumo gerado e salvo com sucesso"})

with open("prompts/prompt_questions.txt", "r") as f:
    original_prompt_template = f.read()

def get_utterances_last_n_seconds(data, n_seconds):
    if not data.get("utterances"):
        return []
    cutoff_time = max(utt["end"] for utt in data["utterances"]) - n_seconds
    return [utt for utt in data["utterances"] if utt["end"] >= cutoff_time]

def append_transcript_to_prompt(prompt, utterances):
    transcript_text = "\n".join(
        [f"Speaker {utt['speaker']}: {utt['text']}" for utt in utterances]
    )
    return prompt + "\n\n" + transcript_text

async def save_transcript_to_db(id: int, transcript_data: dict):
    conn = get_db_connection()
    cursor = conn.cursor()
    transcript_json_text = json.dumps(transcript_data)
    cursor.execute(
        "UPDATE interviews SET transcript = ? WHERE id = ?", (transcript_json_text, id)
    )
    conn.commit()
    conn.close()

def convert_pcm_to_wav(pcm_path: str, wav_path: str, sample_rate: int = 16000, channels: int = 1, sample_width: int = 2):
    """Converte arquivo PCM para WAV adicionando o header"""
    if not os.path.exists(pcm_path) or os.path.getsize(pcm_path) == 0:
        return False
    
    try:
        with open(pcm_path, 'rb') as pcm_file:
            pcm_data = pcm_file.read()
        
        # Calcular tamanhos
        data_size = len(pcm_data)
        file_size = 36 + data_size
        
        # Criar header WAV
        with open(wav_path, 'wb') as wav_file:
            # RIFF header
            wav_file.write(b'RIFF')
            wav_file.write(struct.pack('<I', file_size))
            wav_file.write(b'WAVE')
            
            # fmt chunk
            wav_file.write(b'fmt ')
            wav_file.write(struct.pack('<I', 16))  # fmt chunk size
            wav_file.write(struct.pack('<H', 1))   # audio format (1 = PCM)
            wav_file.write(struct.pack('<H', channels))
            wav_file.write(struct.pack('<I', sample_rate))
            wav_file.write(struct.pack('<I', sample_rate * channels * sample_width))  # byte rate
            wav_file.write(struct.pack('<H', channels * sample_width))  # block align
            wav_file.write(struct.pack('<H', sample_width * 8))  # bits per sample
            
            # data chunk
            wav_file.write(b'data')
            wav_file.write(struct.pack('<I', data_size))
            wav_file.write(pcm_data)
        
        return True
    except Exception as e:
        print(f"Error converting PCM to WAV: {e}")
        return False

@router.post("/{id}/transcribe_audio_file")
async def transcribe_audio_file(id: int):
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT audio_file FROM interviews WHERE id = ?", (id,))
    row = cursor.fetchone()
    if not row or not row["audio_file"]:
        conn.close()
        raise HTTPException(status_code=404, detail="Audio file not found for this interview")

    audio_path = row["audio_file"]
    print(f"[DEBUG] Audio path: {audio_path}")

    if not os.path.exists(audio_path):
        conn.close()
        raise HTTPException(status_code=404, detail="Audio file not found on server")

    try:
        print(f"[DEBUG] Starting transcription for interview {id}")
        config = aai.TranscriptionConfig(
            language_code="pt",
            speaker_labels=True,
            speakers_expected=2,
        )

        print("[DEBUG] Sending to AssemblyAI...")
        transcript = aai.Transcriber(config=config).transcribe(audio_path)
        print(f"[DEBUG] Transcription status: {transcript.status}")

        if transcript.status == "error":
            raise HTTPException(status_code=500, detail=f"Transcription failed: {transcript.error}")

        utt_list = []
        for utt in transcript.utterances:
            utt_dict = {
                "speaker": utt.speaker,
                "text": utt.text,
                "start": utt.start,
                "end": utt.end
            }
            utt_list.append(utt_dict)

        transcript_json = json.dumps(utt_list)

        cursor.execute(
            "UPDATE interviews SET transcript = ? WHERE id = ?", (transcript_json, id)
        )
        conn.commit()

    except HTTPException:
        conn.close()
        raise
    except Exception as e:
        conn.close()
        print(f"[ERROR] Exception type: {type(e).__name__}")
        print(f"[ERROR] Exception message: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")

    conn.close()
    return JSONResponse(content={"id": id, "message": "Audio transcribed and transcript saved successfully", "transcript": utt_list})

@router.websocket("/ws/transcribe")
async def websocket_transcribe(websocket: WebSocket, id: int = Query(...)):
    await websocket.accept()

    transcript_data = {"utterances": []}

    date = datetime.now().isoformat()
    audio_filename = f"interview_{id}_{datetime.now().strftime('%Y%m%d%H%M%S')}.wav"
    upload_dir = "uploads"
    os.makedirs(upload_dir, exist_ok=True)
    audio_path = os.path.join(upload_dir, audio_filename)

    audio_file = await aiofiles.open(audio_path, 'wb')

    config = aai.TranscriptionConfig(
            language_code="pt",
            speaker_labels=True,
            speakers_expected=2,
        )
    
    # Substituir StreamingClient por conexão WebSocket direta do AssemblyAI
    streaming_client = None
    try:
        # Conectar ao AssemblyAI Universal Streaming API v3
        # Parâmetros na URL - speaker_labels pode não estar disponível na v3, mas tentamos
        # A API v3 pode detectar speakers automaticamente sem configuração explícita
        assemblyai_url = f"wss://streaming.assemblyai.com/v3/ws?sample_rate=16000&encoding=pcm_s16le&speech_model=universal-streaming-multilingual"
        # A biblioteca websockets usa additional_headers ao invés de extra_headers
        streaming_client = await websockets.connect(
            assemblyai_url, 
            additional_headers={"Authorization": ASSEMBLYAI_API_KEY}
        )
        print("[DEBUG] Conectado ao AssemblyAI Universal Streaming v3")
    except Exception as e:
        print(f"[WARNING] Could not connect to AssemblyAI Realtime: {e}")
        # Fechar conexão se foi criada mas falhou
        if streaming_client:
            try:
                await streaming_client.close()
            except:
                pass
        streaming_client = None

    try:
        stop_event = asyncio.Event()

        async def send_audio():
            while True:
                audio_chunk = await websocket.receive_bytes()
                await audio_file.write(audio_chunk)
                if streaming_client:
                    # Enviar áudio binário diretamente para AssemblyAI v3 (sem JSON)
                    # O v3 aceita áudio PCM raw diretamente via WebSocket
                await streaming_client.send(audio_chunk)

        async def receive_transcripts():
            if not streaming_client:
                return
            # Rastrear último turn ativo por speaker para agrupar atualizações
            last_active_turn = {}  # {speaker: turn_id}
            turn_counter = {}  # {speaker: counter} para gerar IDs únicos
            finalize_timers = {}  # {speaker: asyncio.Task} para timers de finalização
            last_text = {}  # {turn_id: texto_anterior} para calcular diff
            
            async def finalize_turn(speaker, turn_id, delay=4.0):
                """Finaliza um turn após X segundos de silêncio"""
                try:
                    await asyncio.sleep(delay)
                    # Após 4 segundos, marcar como final e salvar no transcript_data
                    print(f"[DEBUG] Finalizando turn {turn_id} do speaker {speaker} após {delay}s de silêncio")
                    
                    # Salvar no transcript_data
                    if turn_id in last_text:
                        final_text = last_text[turn_id]
                        transcript_data["utterances"].append({
                            "speaker": speaker,
                            "text": final_text,
                            "start": 0,  # Será atualizado se necessário
                            "end": 0
                        })
                    
                    # Enviar mensagem de finalização
                    await websocket.send_json({
                        "transcript_finalize": {
                            "id": turn_id,
                            "speaker": speaker
                        }
                    })
                    
                    # Limpar do active turns para forçar criação de novo ID na próxima frase
                    if speaker in last_active_turn:
                        del last_active_turn[speaker]
                    if turn_id in last_text:
                        del last_text[turn_id]
                except asyncio.CancelledError:
                    # Timer foi cancelado (nova palavra chegou antes de 4s)
                    print(f"[DEBUG] Timer cancelado para turn {turn_id} - pessoa voltou a falar")
                    pass
            
            async for message in streaming_client:
                try:
                    data = json.loads(message)
                    msg_type = data.get("type")
                    
                    # Processar mensagens do Universal Streaming v3
                    if msg_type == "Begin":
                        # Sessão iniciada
                        print(f"Session started: {data.get('id')}")
                    elif msg_type == "Turn":
                        # Transcrição recebida
                        text = data.get("transcript", "")
                        if text:
                            # Debug: log da mensagem completa recebida
                            print(f"[DEBUG] Turn recebido - Data keys: {list(data.keys())}, Text: {text[:50]}...")
                            
                            # Universal Streaming v3 pode retornar speaker de diferentes formas
                            # Tentar diferentes campos possíveis
                            speaker = data.get("speaker") or data.get("speaker_label") or data.get("speaker_id") or "A"
                            # Se for string, converter para maiúscula para consistência
                            if isinstance(speaker, str):
                                speaker = speaker.upper()
                            # Se for número, converter para letra (0->A, 1->B)
                            elif isinstance(speaker, (int, float)):
                                speaker = "A" if speaker == 0 else "B"
                            
                            # Timestamps no formato Turn
                            start = (data.get("start") or 0) / 1000
                            end = (data.get("end") or 0) / 1000
                            
                            # Verificar se é continuação da frase anterior ou nova frase
                            is_new_turn = False
                            if speaker in last_active_turn:
                                turn_id = last_active_turn[speaker]
                                previous_text = last_text.get(turn_id, "")
                                
                                # Se o novo texto não começa com o anterior, é uma nova frase
                                if previous_text and not text.startswith(previous_text):
                                    print(f"[DEBUG] Novo texto não continua o anterior - criando nova frase")
                                    is_new_turn = True
                            else:
                                is_new_turn = True
                            
                            # Se é nova frase, criar novo turn_id
                            if is_new_turn:
                                # Salvar frase anterior (se existir)
                                if speaker in last_active_turn:
                                    old_turn_id = last_active_turn[speaker]
                                    if old_turn_id in last_text:
                                        final_text = last_text[old_turn_id]
                                        transcript_data["utterances"].append({
                                            "speaker": speaker,
                                            "text": final_text,
                                            "start": 0,
                                            "end": 0
                                        })
                                        # Enviar finalização da frase anterior
                                        await websocket.send_json({
                                            "transcript_finalize": {
                                                "id": old_turn_id,
                                                "speaker": speaker
                                            }
                                        })
                                        del last_text[old_turn_id]
                                
                                # Criar nova frase
                                if speaker not in turn_counter:
                                    turn_counter[speaker] = 0
                                turn_counter[speaker] += 1
                                turn_id = f"{speaker}_{turn_counter[speaker]}"
                                last_active_turn[speaker] = turn_id
                                last_text[turn_id] = ""
                                print(f"[DEBUG] Criando nova frase: {turn_id}")
                            
                            # Cancelar timer anterior deste speaker (se existir)
                            if speaker in finalize_timers:
                                finalize_timers[speaker].cancel()
                            
                            # Criar novo timer de 4 segundos para finalizar
                            finalize_timers[speaker] = asyncio.create_task(
                                finalize_turn(speaker, turn_id, delay=4.0)
                            )
                            
                            # Calcular apenas as palavras novas (diff)
                            previous_text = last_text.get(turn_id, "")
                            new_words = text[len(previous_text):] if text.startswith(previous_text) else text
                            last_text[turn_id] = text
                            
                            # Debug: log do speaker recebido
                            print(f"[DEBUG] Turn parcial - Speaker: {speaker}, ID: {turn_id}, New words: '{new_words}'")
                            
                            utt_dict = {
                                "id": turn_id,
                                "speaker": speaker,
                                "text": text,  # Texto completo (para referência)
                                "new_words": new_words,  # Apenas palavras novas
                                "start": start,
                                "end": end,
                                "is_final": False
                            }
                            
                            # Enviar atualização parcial para o frontend
                            print(f"[DEBUG] Enviando palavras novas para frontend: '{new_words[:30]}...'")
                            await websocket.send_json({"transcript_update": [utt_dict]})
                    elif msg_type == "Termination":
                        # Sessão terminada
                        print(f"Session terminated: {data.get('audio_duration_seconds')}s")
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    print(f"Error processing transcription: {e}")

        async def periodic_gpt_analysis():
            while not stop_event.is_set():
                await asyncio.sleep(40)
                last_utts = get_utterances_last_n_seconds(transcript_data, 50)
                if not last_utts:
                    continue
                prompt_to_send = append_transcript_to_prompt(original_prompt_template, last_utts)
                response = await openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "user", "content": prompt_to_send},
                    ],
                    response_format={ "type": "json_object" },
                    max_tokens=200,
                    temperature=0.5
                )
                gpt_message = response.choices[0].message.content.strip()
                await websocket.send_json({"gpt_response": gpt_message})

        await asyncio.gather(
            send_audio(),
            receive_transcripts(),
            periodic_gpt_analysis()
        )

    except WebSocketDisconnect:
        stop_event.set()
        await save_transcript_to_db(id, transcript_data)
        if streaming_client:
            try:
                # Enviar mensagem de Terminate antes de fechar
                await streaming_client.send(json.dumps({"type": "Terminate"}))
            except:
                pass
            try:
        await streaming_client.close()
            except:
                pass
    except Exception as e:
        stop_event.set()
        if streaming_client:
            try:
                # Enviar mensagem de Terminate antes de fechar
                await streaming_client.send(json.dumps({"type": "Terminate"}))
            except:
                pass
            try:
        await streaming_client.close()
            except:
                pass
        print(f"WebSocket error: {e}")
    finally:
        # Garantir que a conexão seja fechada no finally também
        if streaming_client:
            try:
                await streaming_client.close()
            except:
                pass
        await audio_file.close()
        
        # Converter PCM para WAV para compatibilidade com player de áudio
        if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
            wav_path = audio_path.replace('.wav', '_final.wav')
            if convert_pcm_to_wav(audio_path, wav_path):
                # Substituir arquivo PCM por WAV
                try:
                    os.replace(wav_path, audio_path)
                except Exception as e:
                    print(f"Error replacing PCM with WAV: {e}")
        
        # Se não teve transcrição em tempo real, fazer transcrição completa
        if len(transcript_data["utterances"]) == 0 and os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
            try:
                transcriber = aai.Transcriber(config=config)
                transcript = transcriber.transcribe(audio_path)
                if transcript.status == "completed" and transcript.utterances:
                    utt_list = []
                    for utt in transcript.utterances:
                        utt_dict = {
                            "speaker": utt.speaker,
                            "text": utt.text,
                            "start": utt.start,
                            "end": utt.end,
                        }
                        utt_list.append(utt_dict)
                    transcript_data["utterances"] = utt_list
                    await save_transcript_to_db(id, transcript_data)
            except Exception as e:
                print(f"Error transcribing final audio: {e}")
        elif len(transcript_data["utterances"]) > 0:
            await save_transcript_to_db(id, transcript_data)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(
            "UPDATE interviews SET audio_file = ?, date = ? WHERE id = ?", (audio_path, date, id)
        )
        conn.commit()
        conn.close()
