## Visão geral

Este documento descreve a arquitetura da plataforma Compass, combinando a descrição do `README.md` com o fluxograma fornecido (entrevista → áudio → transcrição → resumo padronizado + análises). O objetivo é capturar os componentes, o fluxo de dados e contratos mínimos para os artefatos principais (transcrição, resumo padronizado, análise de perfil e geração de perguntas).

Obs: alguns arquivos de schema/templates no repositório estavam vazios (por exemplo `server/src/schemas/summary.schema.json` e `ai/templates/summary.json`). Aqui faço suposições razoáveis sobre os formatos de entrada/saída; se preferir, posso ajustar os contratos depois com exemplos reais.

## Componentes principais

- Cliente Web (UI)
  - Popup/Interface que o entrevistador usa para subir áudio, ver sugestões e revisar resumos.
  - Envia metadados da entrevista (perguntas pré-definidas, perfil ideal) e arquivos de áudio.

- API Server (Node)
  - Endpoints HTTP para upload de áudio, solicitação de transcrição, geração de resumo, análise e armazenamento.
  - Serviços: transcribe.service, summarize.service, storage.service.

- Serviço de Transcrição
  - Recebe o áudio, executa STT (Speech-to-Text) e devolve a transcrição segmentada por tempo/pergunta.
  - Pode ser um provedor externo (Google, Azure, OpenAI, Whisper local) ou um componente do servidor.

- Serviço de Resumo / IA
  - Consome a transcrição e produz um "resumo padronizado" com perguntas, respostas, pontos positivos/negativos e competências inferidas.
  - Também gera sugestões de perguntas e uma análise de compatibilidade com o "perfil ideal" informado pelo entrevistador.

- Armazenamento
  - Banco de dados para salvar metadados da entrevista, transcrições e resumos padronizados.
  - Storage de objetos para armazenar os áudios brutos (S3 / provider equivalente).

- Dashboard / Repositório de perfis
  - Área onde o entrevistador registra o "perfil ideal" (texto livre) e consulta rankings de candidatos.

## Fluxo de dados (mapeado ao fluxograma)

1. Pré-entrevista: entrevistador fornece
   - cargo, perguntas pré-definidas, vagas, competências necessárias e descrição do perfil ideal
   -> armazenado como `InterviewTemplate` ou `Profile` no sistema.

2. Durante a entrevista:
   - Áudio é gravado e enviado ao sistema (ou enviado ao final).
   - Áudio também pode alimentar uma análise comportamental em paralelo (voz, pausas, tom).

3. Transcrição:
   - Áudio -> Serviço de Transcrição -> Transcrição segmentada.
   - Transcrição devolvida ao servidor e persistida.

4. Geração de resumo padronizado (IA):
   - Transcrição + `InterviewTemplate` (perguntas pré-definidas) + `Profile` (perfil ideal) -> Serviço de Resumo/IA
   - Produz: resumo padronizado (perguntas e respostas), pontos positivos/negativos, competências com porcentagem de adequação, e sugestões de perguntas.

5. Análise de perfil e ranqueamento:
   - O texto do perfil ideal é usado como referência para ranquear candidatos por similaridade.
   - Resultado: score / ranking por candidato.

6. Entregáveis ao usuário:
   - Resumo padronizado para cada entrevista
   - Lista de perguntas sugeridas (novas ou modificações)
   - Ranking de candidatos com relação ao perfil ideal
   - (Opcional) Relatório de análise comportamental

## Contratos de dados (inferencia / proposta)

1) InterviewTemplate (entrada pré-entrevista)

{
  "id": "string",
  "title": "string",
  "role": "string",
  "questions": [
    { "id": "q1", "text": "string", "type": "behavioral|technical|other" }
  ],
  "required_competencies": ["comunicação", "liderança"],
  "description_profile_ideal": "texto livre"
}

2) Audio upload / metadata

{
  "id": "string",
  "interviewTemplateId": "string",
  "candidate": { "name": "string", "id": "string" },
  "audio_url": "s3://... or https://...",
  "duration": 12345,
  "recorded_at": "ISO8601"
}

3) Transcription (serviço de STT)

{
  "id": "string",
  "audio_id": "string",
  "text": "string",
  "segments": [
    { "start": 0.0, "end": 12.3, "text": "resposta a pergunta X", "speaker": "interviewee|interviewer" }
  ]
}

4) Summary (resumo padronizado — saída IA)

{
  "id": "string",
  "interview_id": "string",
  "questions": [
    {
      "question_id": "q1",
      "question_text": "string",
      "answer_text": "string",
      "highlights": ["ponto positivo", "ponto negativo"]
    }
  ],
  "pros": ["lista de pontos positivos"],
  "cons": ["lista de pontos a melhorar"],
  "competencies": [
    { "name": "comunicacao", "score": 0.87 }
  ],
  "suggested_questions": [
    { "text": "Você pode me dar um exemplo de ...", "reason": "clarificar projeto X" }
  ],
  "generated_at": "ISO8601"
}

Observação: os campos acima são sugestivos. Se tivermos o `ai/templates/summary.json` preenchido depois, atualizamos o contrato.

## API Endpoints (proposta)

- POST /api/interviews - cria entrevista / template
- POST /api/interviews/:id/audio - upload de áudio
- POST /api/interviews/:id/transcribe - inicia transcrição
- POST /api/interviews/:id/summarize - solicita geração do resumo padronizado
- GET /api/interviews/:id/summary - obtém resumo gerado
- GET /api/profiles - CRUD de perfis ideais
- GET /api/candidates/:id/ranking - ranking vs perfil

## Não-funcionais / Requisitos técnicos

- Latência: a transcrição e o resumo podem ser processados assincronamente; oferecer webhooks ou polling.
- Custo: preferir pipelines que permitam controle de custo (batching, modelos menos caros para pré-processamento).
- Privacidade: áudios e resumos podem conter PII — criptografar em trânsito e repouso, controlar acessos.
- Escalabilidade: usar filas (ex: SQS / Redis streams) para desacoplar upload → transcrição → resumo.

## Erros e casos de borda

- Áudio com má qualidade: retry de transcrição com modelos alternativos ou sinalizar para transcrição manual.
- Falha no serviço de IA: fallback para resumo mínimo (ex: resumo com extração simples de frases-chave).
- Inconsistência entre timestamps e segmentos: validar e normalizar timestamps na etapa de ingestão.

## Observações sobre o fluxograma enviado

- O fluxograma apresenta claramente os blocos:
  - Input pré-entrevista (perfil ideal, perguntas, competências)
  - ÁUDIO -> TRANSCRIÇÃO -> resumo padronizado
  - A partir da transcrição também há: análise comportamental (voz) e sugestão de perguntas
  - Análise de perfil (rankear) consome tanto o perfil ideal quanto a transcrição/resumo

Essas relações estão refletidas nos componentes e nos contratos acima.

## Arquitetura para Chrome Extension + Website

Contexto: a plataforma será composta por uma aplicação web (dashboard/site) que apresenta as features 1 e 2 (resumos padronizados e ranqueamento/analise de perfil) e por uma extensão Chrome que atua durante chamadas no Google Meet para entregar a feature 3 (sugestões de perguntas em tempo real e UI durante a call). Abaixo descrevo componentes, fluxo e opções técnicas específicas para esse modelo híbrido.

Componentes da solução híbrida

- Chrome Extension
  - Manifest (MV3 recomendado): define permissões, host permissions para `meet.google.com`, `activeTab`, `tabCapture`/`scripting` se necessário.
  - Content Script: injetado na página do Meet para observar o DOM, detectar eventos da chamada (join/leave), e (quando aplicável) interceptar ou criar pontos de acesso ao fluxo de áudio/WebRTC da página.
  - Background (service worker): mantém conexões WebSocket com o backend, faz autenticação e coordena uploads/streams.
  - Popup / DevTools / Sidebar: UI local da extensão (opcional) com controles de gravação, botão de iniciar/transmitir, e permissões.
  - In-page UI (injected): bolhas, sugestões e prompts mostrados dentro da interface do Meet (feature 3) para o entrevistador.

- Website / Dashboard
  - Mostra histórico de entrevistas, resumos padronizados e rankings (features 1 e 2).
  - CRUD para `InterviewTemplate` e `Profile` (perfil ideal).
  - Área para revisar resumos gerados, exportar e gerenciar políticas de retenção/privacidade.

- Backend / Serviços (conforme documento anterior)
  - Endpoints REST + WebSocket para ingestão de áudio, transcrição em tempo real ou por arquivo, e geração de resumos.

Comunicação entre extensão e site

- Autenticação: token JWT emitido pelo site (login do entrevistador) e armazenado no extension storage. A extensão envia esse token ao backend em WebSocket/REST.
- Canal em tempo real: usar WebSocket (ou WebRTC data channel se preferir) entre a extensão e o servidor para enviar pequenos chunks de áudio/transcrição intermediária e receber sugestões em baixa latência.
- Upload assíncrono: quando a chamada terminar, a extensão pode enviar o arquivo completo ao endpoint `/api/interviews/:id/audio` para processamento offline (batch).

Captura de áudio — opções e trade-offs

Capturar áudio completo de uma reunião (todas as vozes) no browser tem limitações e exigirá permissões e/ou técnicas específicas. Principais estratégias:

1) Capturar o microfone do usuário (fácil, confiável)
   - Uso: getUserMedia({ audio: true }) a partir da extensão (ou a página) — captura apenas o áudio do entrevistador local.
   - Prós: pouco trabalho, funciona em MV3 com permissões padrão e indica consentimento do usuário.
   - Contras: não captura a voz remota dos entrevistados; limita a análise a apenas um lado da conversa.

2) Capturar áudio da aba / sistema (captura da reunião completa)
   - chrome.tabCapture / chrome.desktopCapture ou getDisplayMedia com audio (quando usuário compartilha a aba) — permite pegar o áudio do Meet (participantes remotos + local dependendo do método).
   - Prós: captura ambos os lados da conversa, melhor para transcrição completa.
   - Contras: requer permissões explícitas e pode precisar que o usuário compartilhe a aba/janela; em alguns modos não funciona em todas as plataformas; implementação mais complexa.

3) Interceptar WebRTC (avançado)
   - Injetar código que amarra o RTCPeerConnection para duplicar/escutar streams (é técnica intrusiva e frágil frente a mudanças do Meet).
   - Prós: captura nativamente o stream usado pelo Meet.
   - Contras: alto risco de quebrar com atualizações do Meet, questões legais e de privacidade; recomendado apenas se souber lidar com manutenção constante.

Recomendação prática

- Implementação mínima viável (MVP): comece com captura do microfone local + upload completo da gravação ao final da call. Para a captura do áudio remoto (conversa completa) ofereça a opção "capturar aba" que instrui o usuário a compartilhar a aba do Meet (getDisplayMedia or tabCapture) quando a extensão pedir.
- Para sugestões em tempo real (feature 3): envie pequenos fragmentos de áudio ou transcrição local (por SEGUNDO ou por 3–5s) por WebSocket ao backend que roda um modelo de baixa-latência (ou um pipeline que use ASR em tempo real) e retorne prompts/sugestões em <1–2s.

Fluxo durante uma call (demo):

1. Entrevistador faz login no site e autoriza a extensão (token salvo localmente).
2. Ao entrar na call, content script detecta a reunião e o entrevistador clica em "Iniciar gravação / sugestões" na extensão.
3. Extensão pede permissões (microfone e opcional "capturar aba").
4. Extensão abre WebSocket com o backend e envia pequenos chunks de áudio (ou transcrição local) para produção de sugestões em tempo real.
5. Backend envia sugestões para a extensão, que as exibe no overlay do Meet (feature 3).
6. Ao final da call, extensão faz upload do áudio completo e metadados para o endpoint `/api/interviews/:id/audio` e aciona `/summarize` para gerar o resumo padronizado (features 1 e 2) que o site exibirá.

Permissões, privacidade e conformidade

- Consentimento: sempre peça consentimento explícito antes de gravar a call; mostre avisos visíveis na UI da extensão.
- Dados sensíveis: criptografar áudios em repouso e em trânsito; oferecer configurações de anonimização (remover nomes, PII) se necessário.
- Escopo de permissões: declare apenas as permissões mínimas no `manifest.json` e explique por que cada uma é necessária.
- Logging & retenção: permita que administradores definam tempo de retenção dos áudios/resumos.

Observações finais

Esta seção foi adicionada para alinhar a arquitetura à sua exigência: extensão Chrome que atua em chamadas do Meet e um site que apresenta as funcionalidades 1 e 2 enquanto a extensão fornece a 3 em tempo real. Se quiser, eu adapto o `extension/manifest.json` do repositório para MV3 com permissões sugeridas e crio a estrutura inicial de content script + background + in-page UI; também posso gerar um exemplo de WebSocket protocol/contract (mensagens de áudio chunk, ack, sugestões) para implementar a camada em tempo real.
## Próximos passos recomendados

1. Preencher o `ai/templates/summary.json` com um exemplo real de saída de IA (um ou dois exemplos de entrevistas) — eu posso gerar exemplos se você quiser.
2. Implementar um schema JSON formal (`server/src/schemas/summary.schema.json`) com base no contrato acima e validar respostas da IA.
3. Criar um diagrama em Mermaid ou draw.io para anexar ao `docs/` (posso gerar uma versão Mermaid para inclusão direta no README).
4. Definir política de retenção/criptografia para áudios e resumos.

Se quiser, eu já transformo a seção "Fluxo de dados" em um diagrama Mermaid e adiciono ao repositório como `docs/arquitetura.mmd` ou incorporar dentro deste arquivo. Qual formato prefere? 
